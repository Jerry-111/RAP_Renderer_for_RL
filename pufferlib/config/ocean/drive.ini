[base]
package = ocean
env_name = puffer_drive
policy_name = Drive
rnn_name = Recurrent

[vec]
num_workers = 16
num_envs = 16
batch_size = 4
; backend = Serial

[policy]
input_size = 64
hidden_size = 256

[rnn]
input_size = 256
hidden_size = 256

[env]
num_agents = 1024
; Options: discrete, continuous
action_type = discrete
; Options: classic, jerk
dynamics_model = classic
reward_vehicle_collision = -0.5
reward_offroad_collision = -0.5
dt = 0.1
reward_goal = 1.0
reward_goal_post_respawn = 0.25
; Meters around goal to be considered "reached"
goal_radius = 2.0
; Max target speed in m/s for the agent to maintain towards the goal
goal_speed = 100.0
; What to do when the goal is reached. Options: 0:"respawn", 1:"generate_new_goals", 2:"stop"
goal_behavior = 0
; Determines the target distance to the new goal in the case of goal_behavior = generate_new_goals.
; Large numbers will select a goal point further away from the agent's current position.
goal_target_distance = 30.0
; Options: 0 - Ignore, 1 - Stop, 2 - Remove
collision_behavior = 0
; Options: 0 - Ignore, 1 - Stop, 2 - Remove
offroad_behavior = 0
; Number of steps before
episode_length = 91
resample_frequency = 910
termination_mode = 1 # 0 - terminate at episode_length, 1 - terminate after all agents have been reset
map_dir = "resources/drive/binaries/training"
num_maps = 10000
; Determines which step of the trajectory to initialize the agents at upon reset
init_steps = 0
; Options: "control_vehicles", "control_agents", "control_wosac", "control_sdc_only"
control_mode = "control_vehicles"
; Options: "created_all_valid", "create_only_controlled"
init_mode = "create_all_valid"

[train]
seed=42
total_timesteps = 2_000_000_000
; learning_rate = 0.02
; gamma = 0.985
anneal_lr = True
; Needs to be: num_agents * num_workers * BPTT horizon
batch_size = 524288
minibatch_size = 32768
max_minibatch_size = 32768
bptt_horizon = 32
adam_beta1 = 0.9
adam_beta2 = 0.999
adam_eps = 1e-8
clip_coef = 0.2
ent_coef = 0.005
gae_lambda = 0.95
gamma = 0.98
learning_rate = 0.003
max_grad_norm = 1
prio_alpha = 0.8499999999999999
prio_beta0 = 0.8499999999999999
update_epochs = 1
vf_clip_coef = 0.1999999999999999
vf_coef = 2
vtrace_c_clip = 1
vtrace_rho_clip = 1
checkpoint_interval = 1000
; Rendering options
render = True
render_interval = 1000
; If True, show exactly what the agent sees in agent observation
obs_only = True
; Show grid lines
show_grid = True
; Draws lines from ego agent observed ORUs and road elements to show detection range
show_lasers = False
; Display human xy logs in the background
show_human_logs = False
; If True, zoom in on a part of the map. Otherwise, show full map
zoom_in = True
; Options: List[str to path], str to path (e.g., "resources/drive/training/binaries/map_001.bin"), None
render_map = none

[eval]
eval_interval = 1000
; Path to dataset used for evaluation
map_dir = "resources/drive/binaries/training"
; Number of scenarios to process per batch
wosac_batch_size = 32
; Target number of unique scenarios perform evaluation in
wosac_target_scenarios = 64
; Total pool of scenarios to sample from
wosac_scenario_pool_size = 1000
; Max batches, used as a timeout to prevent an infinite loop
wosac_max_batches = 100
backend = PufferEnv
; WOSAC (Waymo Open Sim Agents Challenge) evaluation settings
; If True, enables evaluation on realism metrics each time we save a checkpoint
wosac_realism_eval = False
; Number of policy rollouts per scene
wosac_num_rollouts = 32
; When to start the simulation
wosac_init_steps = 10
; Control everything valid at init in the scene
wosac_control_mode = "control_wosac"
; Create everything in valid at init the scene
wosac_init_mode = "create_all_valid"
; Stop when reaching the goal
wosac_goal_behavior = 2
; Can shrink goal radius for WOSAC evaluation
wosac_goal_radius = 2.0
wosac_sanity_check = False
; Only return aggregate results across all scenes
wosac_aggregate_results = True
; If True, enable human replay evaluation (pair policy-controlled agent with human replays)
human_replay_eval = False
; Control only the self-driving car
human_replay_control_mode = "control_sdc_only"
; Number of scenarios for human replay evaluation equals the number of agents
human_replay_num_agents = 16

[sweep.train.learning_rate]
distribution = log_normal
min = 0.001
mean = 0.003
max = 0.005
scale = auto

[sweep.train.ent_coef]
distribution = log_normal
min = 0.001
mean = 0.005
max = 0.03
scale = auto

[sweep.train.gamma]
distribution = log_normal
min = 0.97
mean = 0.98
max = 0.999
scale = auto

[sweep.train.gae_lambda]
distribution = log_normal
min = 0.95
mean = 0.98
max = 0.999
scale = auto

[controlled_exp.train.goal_speed]
values = [10, 20, 30, 3]

[controlled_exp.train.ent_coef]
values = [0.001, 0.005, 0.01]

[controlled_exp.train.seed]
values = [42, 55, 1]
